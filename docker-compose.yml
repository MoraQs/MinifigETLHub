services:
  # Airflow Webserver
  airflow:
    image: apache/airflow:2.7.2
    container_name: airflow_etl
    restart: always
    environment:
      - AIRFLOW_CORE_EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}
      - AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags
    env_file:
      - .env      # loading env_file to be used within airflow to run DAGs
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./scripts:/opt/airflow/scripts
      - ./data:/opt/airflow/data
    ports:
      - "8080:8080"
    command: ["webserver"]

  # Airflow Scheduler
  airflow_scheduler:
    image: apache/airflow:2.7.2
    container_name: airflow_scheduler
    restart: "no"
    depends_on:
      - airflow
    env_file:
      - .env
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./scripts:/opt/airflow/scripts
    command: ["scheduler"]

  # Python ETL Application Container (Dockerfile)
  etl_app:
    build: .
    env_file:
      - .env
    volumes:
      - .:/app  # Syncs local project directory with /app inside the container
    depends_on:
      - airflow_scheduler
    command: ["sh", "-c", "python scripts/api_call.py && tail -f /dev/null"]  # Keeps container running

  # Jupyter for transformations
  jupyter:
    image: jupyter/base-notebook:latest
    container_name: jupyter_notebook
    restart: always
    ports:
      - "8888:8888"
    volumes:
      - ./notebooks:/home/jovyan/work
      - ./data:/home/jovyan/data       # Mount the data directory
    environment:
      - JUPYTER_ENABLE_LAB=yes
    env_file:
      - .env     # loading env_file to be used within jupyterLab for transformation